---
title: "Predicting Internet Affordability Perception in Zimbabwe"
author: "Kris Tadzembwa"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# Load all necessary libraries
library(tidyverse)
library(caret)
library(randomForest)
library(recipes)
library(DALEX)
library(pROC)
library(plotly)
library(reshape2)
```

# 1. Introduction

This analysis aims to build a predictive model for internet affordability perception in Zimbabwe based on survey data. The goal is to identify key factors that influence whether individuals find internet services affordable or not.

# 2. Data Exploration & Preprocessing

```{r load_data}
# Load the data
data <- read.csv("internet_data.csv", stringsAsFactors = FALSE)

# Basic exploration
cat("Dataset dimensions:", dim(data), "\n")
cat("\nDataset structure:\n")
str(data)
cat("\nSummary statistics:\n")
summary(data)
```

```{r preprocessing}
# Clean column names
colnames(data) <- make.names(colnames(data))

# Handle missing values and "Prefer not to say" entries
data[data == "Prefer not to say" | data == "" | data == "Mmm" | data == "Like what"] <- NA

# Convert categorical variables to factors
categorical_vars <- c("X..Age.group", "X..Education..", "X..Occupation..", 
                      "Location..Area.", "province", "X..Primary.internet.provider...", 
                      "X..Technology.type..", "X...Primary.device..", 
                      "X..Usage.location..", "X..Affordability..")

data[categorical_vars] <- lapply(data[categorical_vars], as.factor)

# Create target variable
data$target <- ifelse(data$X..Affordability.. %in% c("very affordable", "affordable"), 1, 0)
data$target <- as.factor(data$target)

cat("Target variable distribution:\n")
table(data$target)
```

# 3. Feature Engineering

```{r feature_engineering}
# Feature 1: Internet Satisfaction Score
data$satisfaction_score <- (data$Speed.Satisfaction + data$Reliability.rating) / 2

# Feature 2: App Performance Index
app_cols <- c("App.performance....Whatsapp.", "App.performance....YouTube.", 
              "App.performance....Voice.Calls.", "App.performance....Video.Calls.")

# Convert app performance to numeric
for(col in app_cols) {
    data[[col]] <- case_when(
        grepl("works well", data[[col]], ignore.case = TRUE) ~ 3,
        grepl("sometimes slow", data[[col]], ignore.case = TRUE) ~ 2,
        grepl("often fails", data[[col]], ignore.case = TRUE) ~ 1,
        grepl("doesnt work|doesn't work", data[[col]], ignore.case = TRUE) ~ 0,
        TRUE ~ NA_real_
    )
}

data$app_performance_index <- rowMeans(data[app_cols], na.rm = TRUE)

# Feature 3: Digital Engagement Score
online_activities <- c("X..Do.you.earn.online...economic.benefit.", 
                       "X..Use.internet.for.business....", 
                       "X..Use.internet.for.job.searching...", 
                       "X..Use.internet.for.education...")

for(col in online_activities) {
    data[[paste0(col, "_binary")]] <- ifelse(data[[col]] == "yes", 1, 0)
}

data$digital_engagement_score <- rowSums(data[paste0(online_activities, "_binary")], na.rm = TRUE)

# Feature 4: Technology Diversity Index
data$tech_diversity <- str_count(data$X..Technology.type.., ";") + 1
data$tech_diversity[is.na(data$X..Technology.type..)] <- 0

# Feature 5: Urban-Income Interaction
data$Monthly.income.numeric <- case_when(
    data$X..Monthly.income.. == "Less than $100" ~ 50,
    data$X..Monthly.income.. == "$100–300" ~ 200,
    data$X..Monthly.income.. == "$300–500" ~ 400,
    data$X..Monthly.income.. == "$500+" ~ 750,
    TRUE ~ NA_real_
)

data$urban_income_interaction <- ifelse(data$Location..Area. == "Urban", 1, 0) * 
    coalesce(data$Monthly.income.numeric, 0)

# Clean column names for easier handling
colnames(data) <- colnames(data) %>%
    str_replace("^X\\.+", "") %>%
    str_replace_all("\\.+", "_") %>%
    str_replace("_+$", "") %>%
    str_to_lower()
```

# 4. Modeling

```{r modeling_setup}
# Define features for modeling
model_features <- c("satisfaction_score", "app_performance_index", 
                    "digital_engagement_score", "tech_diversity", 
                    "urban_income_interaction", "age_group", "education", 
                    "occupation", "location_area", "province", 
                    "primary_internet_provider", "household_size", "target")

# Create model dataset
model_data <- data[model_features]

# Convert household_size to numeric
model_data$household_size <- case_when(
    model_data$household_size == "1" ~ 1,
    model_data$household_size == "2" ~ 2,
    model_data$household_size == "3" ~ 3,
    model_data$household_size == "4" ~ 4,
    model_data$household_size == "5" ~ 5,
    model_data$household_size == "more than 5" ~ 6,
    TRUE ~ NA_real_
)

# Remove NAs
model_data <- na.omit(model_data)

# Ensure target has proper factor levels
model_data$target <- factor(model_data$target, levels = c("0", "1"))

# Split data
set.seed(123)
train_index <- createDataPartition(model_data$target, p = 0.8, list = FALSE)
train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

cat("Training data dimensions:", dim(train_data), "\n")
cat("Test data dimensions:", dim(test_data), "\n")
```

```{r recipe_preprocessing}
# Create preprocessing recipe
model_recipe <- recipe(target ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_zv(all_predictors())

# Prepare and apply recipe
prepped_recipe <- prep(model_recipe, training = train_data)
train_baked <- bake(prepped_recipe, new_data = train_data)
test_baked <- bake(prepped_recipe, new_data = test_data)

cat("Preprocessed training data dimensions:", dim(train_baked), "\n")
```

```{r train_models}
# Train Random Forest
set.seed(123)
rf_model <- randomForest(target ~ ., 
                        data = train_baked, 
                        ntree = 500,
                        importance = TRUE)

# Make predictions
rf_pred_prob <- predict(rf_model, test_baked, type = "prob")[, 2]
rf_pred_class <- predict(rf_model, test_baked)

# Calculate metrics
calculate_metrics <- function(actual, predicted_class, predicted_prob) {
    cm <- confusionMatrix(predicted_class, actual)
    roc_obj <- roc(as.numeric(actual)-1, predicted_prob)
    auc_val <- auc(roc_obj)
    
    list(
        accuracy = cm$overall['Accuracy'],
        precision = cm$byClass['Precision'],
        recall = cm$byClass['Recall'],
        f1 = cm$byClass['F1'],
        auc = as.numeric(auc_val)
    )
}

rf_metrics <- calculate_metrics(test_baked$target, rf_pred_class, rf_pred_prob)
```

# 5. Evaluation

```{r evaluation}
# Cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)

cv_data <- train_data
cv_data$target <- factor(cv_data$target, levels = c("0", "1"), labels = c("No", "Yes"))

set.seed(123)
rf_cv <- train(
  model_recipe,
  data = cv_data, 
  method = "rf",
  trControl = ctrl,
  metric = "ROC",
  tuneLength = 3,
  ntree = 200
)

cat("Cross-validation results:\n")
print(rf_cv$results)
```

```{r learning_curve}
# Learning curve function
create_learning_curve <- function(data, formula, recipe_blueprint) {
  n <- nrow(data)
  train_sizes <- seq(10, n-5, by = 5)
  train_scores <- c()
  val_scores <- c()
  
  for (size in train_sizes) {
    subset_data <- data[1:size, ]
    train_idx <- sample(1:nrow(subset_data), 0.8 * nrow(subset_data))
    train_sub <- subset_data[train_idx, ]
    val_sub <- subset_data[-train_idx, ]
    
    if (nrow(val_sub) > 0) {
      prepped <- prep(recipe_blueprint, training = train_sub)
      train_baked <- bake(prepped, new_data = train_sub)
      val_baked <- bake(prepped, new_data = val_sub)
      
      rf_temp <- randomForest(formula, data = train_baked, ntree = 100)
      train_pred <- predict(rf_temp, train_baked)
      train_acc <- mean(train_pred == train_sub$target)
      
      val_pred <- predict(rf_temp, val_baked)
      val_acc <- mean(val_pred == val_sub$target)
      
      train_scores <- c(train_scores, train_acc)
      val_scores <- c(val_scores, val_acc)
    }
  }
  
  data.frame(
    TrainSize = train_sizes[1:length(train_scores)],
    TrainAccuracy = train_scores,
    ValAccuracy = val_scores
  )
}

# Generate learning curve
lc_data <- create_learning_curve(
  data = train_data,
  formula = target ~ .,
  recipe_blueprint = model_recipe
)

# Plot learning curve
ggplot(lc_data, aes(x = TrainSize)) +
  geom_line(aes(y = TrainAccuracy, color = "Training Accuracy")) +
  geom_line(aes(y = ValAccuracy, color = "Validation Accuracy")) +
  labs(title = "Random Forest Learning Curve",
       x = "Training Set Size",
       y = "Accuracy",
       color = "Legend") +
  theme_minimal()
```

# 6. Interpretability

```{r interpretability}
# Create explainer
explainer_rf <- explain(
  model = rf_model,
  data = test_baked[, -which(names(test_baked) == "target")],
  y = as.numeric(as.character(test_baked$target)),
  label = "Random Forest",
  type = "classification"
)

# Feature importance
vi_rf <- model_parts(explainer_rf, type = "difference")
plot(vi_rf, show_boxplots = FALSE) +
  ggtitle("Feature Importance for Random Forest Model")

# Instance-level explanations
set.seed(123)
selected_instances <- sample(1:nrow(test_baked), 2)

for (i in selected_instances) {
  bd_rf <- predict_parts(
    explainer = explainer_rf,
    new_observation = test_baked[i, -which(names(test_baked) == "target")],
    type = "break_down"
  )
  plot(bd_rf) +
    ggtitle(paste("Break-Down Profile for Test Instance", i),
            subtitle = paste("True Class:", test_baked$target[i], 
                            "| Predicted Probability:", round(rf_pred_prob[i], 3)))
}
```

# 7. Conclusion

The Random Forest model achieved an accuracy of `r round(rf_metrics$accuracy, 3)` and AUC of `r round(rf_metrics$auc, 3)` on the test set. The most important features for predicting internet affordability perception were app performance and urban income interaction.

**Key Findings:**
- App performance is the strongest predictor of affordability perception
- Urban residents with higher income have different affordability perceptions
- The model shows good generalization with cross-validation AUC of `r round(max(rf_cv$results$ROC), 3)`

**Deployment Considerations:**
- The preprocessing recipe ensures consistent handling of new data
- Model latency is low, making it suitable for real-time applications
- Potential biases should be monitored, especially across different geographic regions